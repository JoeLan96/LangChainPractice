{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith can generate test cases automatically based on the specifications provided. This can help in generating a comprehensive set of test cases quickly and efficiently.\\n\\n2. Test Data Generation: Langsmith can help in generating test data for various scenarios, including edge cases and boundary conditions. This can ensure that the application is tested thoroughly.\\n\\n3. Test Coverage Analysis: Langsmith can provide insights into the test coverage, helping testers identify areas that need additional testing.\\n\\n4. Regression Testing: Langsmith can help in automating regression testing by generating test cases for new features or changes in the application.\\n\\n5. Performance Testing: Langsmith can generate test cases for performance testing to ensure that the application meets the required performance criteria.\\n\\nOverall, Langsmith can streamline the testing process, improve test coverage, and help in identifying and addressing issues early in the development cycle.' response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 27, 'total_tokens': 209}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n",
      "<class 'str'>\n",
      "Langsmith can be a valuable tool for testing in various ways. Here are some ways how Langsmith can help with testing:\n",
      "\n",
      "1. **Automated Testing**: Langsmith can be used to automate testing tasks, such as generating test data, running test cases, and analyzing test results. This can help in speeding up the testing process and ensuring consistent test execution.\n",
      "\n",
      "2. **Test Data Generation**: Langsmith can generate realistic and diverse test data, which can be used to test the application under different scenarios and edge cases. This can help in uncovering potential bugs and issues in the application.\n",
      "\n",
      "3. **Performance Testing**: Langsmith can simulate large volumes of data and user interactions to test the performance of the application. This can help in identifying performance bottlenecks and optimizing the application for better performance.\n",
      "\n",
      "4. **API Testing**: Langsmith can be used to generate test data for API testing, validate API responses, and automate API testing tasks. This can help in ensuring the reliability and functionality of APIs.\n",
      "\n",
      "5. **Regression Testing**: Langsmith can automate regression testing tasks by generating test data and running test cases on new versions of the application. This can help in ensuring that new changes do not introduce any regressions in the application.\n",
      "\n",
      "Overall, Langsmith can help in improving the efficiency, effectiveness, and coverage of testing activities, leading to higher quality software products.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 加载模型\n",
    "llm = ChatOpenAI(\n",
    "    model_name =\"gpt-3.5-turbo\",\n",
    "    openai_api_base=\"https://api.nextapi.fun/v1\", # 注意，末尾要加 /v1\n",
    "    openai_api_key=\"ak-5q53TviefzOYW8xW2F0gFtS20aO2gT288sPWSD92YQpjqmSJ\",\n",
    ")\n",
    "\n",
    "res = llm.invoke(\"hello\")\n",
    "\n",
    "print(res)\n",
    "\n",
    "\n",
    "# 设置提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "# 创建 Chain\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(type(response))\n",
    "# langchain_core.messages.ai.AIMessage\n",
    "print(response)\n",
    "\n",
    "\n",
    "# 设置输出解析器\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 创建 Chain\n",
    "chain2 = prompt | llm | output_parser\n",
    "\n",
    "# 调用 Chain\n",
    "response2 = chain2.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "\n",
    "print(type(response2))\n",
    "# str\n",
    "print(response2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T09:41:38.307414200Z",
     "start_time": "2024-03-27T09:41:23.155670400Z"
    }
   },
   "id": "9b9a1fc01f9dbe7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## langchain 中转API版本"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e00d4f19403c0887"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import docarray python package. Please install it with `pip install \"langchain[docarray]\"`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32mD:\\programfiles\\Anaconda\\envs\\promptclpy38\\lib\\site-packages\\langchain_community\\vectorstores\\docarray\\base.py:19\u001B[0m, in \u001B[0;36m_check_docarray_import\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdocarray\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     da_version \u001B[38;5;241m=\u001B[39m docarray\u001B[38;5;241m.\u001B[39m__version__\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'docarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrunnables\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RunnableParallel, RunnablePassthrough\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_openai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatOpenAI, OpenAIEmbeddings\n\u001B[1;32m----> 7\u001B[0m vectorstore \u001B[38;5;241m=\u001B[39m \u001B[43mDocArrayInMemorySearch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mharrison worked at kensho\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbears like to eat honey\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mOpenAIEmbeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mopenai_api_base\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://api.nextapi.fun/v1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# 注意，末尾要加 /v1\u001B[39;49;00m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mopenai_api_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mak-5q53TviefzOYW8xW2F0gFtS20aO2gT288sPWSD92YQpjqmSJ\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m retriever \u001B[38;5;241m=\u001B[39m vectorstore\u001B[38;5;241m.\u001B[39mas_retriever()\n\u001B[0;32m     16\u001B[0m template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mAnswer the question based only on the following context:\u001B[39m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;132;01m{context}\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \n\u001B[0;32m     19\u001B[0m \u001B[38;5;124mQuestion: \u001B[39m\u001B[38;5;132;01m{question}\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n",
      "File \u001B[1;32mD:\\programfiles\\Anaconda\\envs\\promptclpy38\\lib\\site-packages\\langchain_community\\vectorstores\\docarray\\in_memory.py:68\u001B[0m, in \u001B[0;36mDocArrayInMemorySearch.from_texts\u001B[1;34m(cls, texts, embedding, metadatas, **kwargs)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_texts\u001B[39m(\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m     53\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DocArrayInMemorySearch:\n\u001B[0;32m     54\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create an DocArrayInMemorySearch store and insert data.\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;124;03m        DocArrayInMemorySearch Vector Store\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 68\u001B[0m     store \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_params\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m     store\u001B[38;5;241m.\u001B[39madd_texts(texts\u001B[38;5;241m=\u001B[39mtexts, metadatas\u001B[38;5;241m=\u001B[39mmetadatas)\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m store\n",
      "File \u001B[1;32mD:\\programfiles\\Anaconda\\envs\\promptclpy38\\lib\\site-packages\\langchain_community\\vectorstores\\docarray\\in_memory.py:39\u001B[0m, in \u001B[0;36mDocArrayInMemorySearch.from_params\u001B[1;34m(cls, embedding, metric, **kwargs)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_params\u001B[39m(\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m     29\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DocArrayInMemorySearch:\n\u001B[0;32m     30\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Initialize DocArrayInMemorySearch store.\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124;03m        **kwargs: Other keyword arguments to be passed to the get_doc_cls method.\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m     \u001B[43m_check_docarray_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocarray\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mindex\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InMemoryExactNNIndex\n\u001B[0;32m     42\u001B[0m     doc_cls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_get_doc_cls(space\u001B[38;5;241m=\u001B[39mmetric, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\programfiles\\Anaconda\\envs\\promptclpy38\\lib\\site-packages\\langchain_community\\vectorstores\\docarray\\base.py:29\u001B[0m, in \u001B[0;36m_check_docarray_import\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     24\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo use the DocArrayHnswSearch VectorStore the docarray \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     25\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion >=0.32.0 is expected, received: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdocarray\u001B[38;5;241m.\u001B[39m__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     26\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo upgrade, please run: `pip install -U docarray`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     27\u001B[0m         )\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m---> 29\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     30\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import docarray python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPlease install it with `pip install \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlangchain[docarray]\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     32\u001B[0m     )\n",
      "\u001B[1;31mImportError\u001B[0m: Could not import docarray python package. Please install it with `pip install \"langchain[docarray]\"`."
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(\n",
    "        openai_api_base=\"https://api.nextapi.fun/v1\", # 注意，末尾要加 /v1\n",
    "        openai_api_key=\"ak-5q53TviefzOYW8xW2F0gFtS20aO2gT288sPWSD92YQpjqmSJ\",\n",
    "    ),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "model = ChatOpenAI(\n",
    "    model_name =\"gpt-3.5-turbo\",\n",
    "    openai_api_base=\"https://api.nextapi.fun/v1\", # 注意，末尾要加 /v1\n",
    "    openai_api_key=\"ak-5q53TviefzOYW8xW2F0gFtS20aO2gT288sPWSD92YQpjqmSJ\",\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:14:19.995510200Z",
     "start_time": "2024-03-27T10:14:19.334395900Z"
    }
   },
   "id": "f5c2424d2968e312"
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## langchain 官方API版本"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4015cba86fb285a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# 如果你需要通过代理端口访问，你需要如下配置\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"HTTP_PROXY\"] = 'http://127.0.0.1:7890'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:29:58.267934200Z",
     "start_time": "2024-03-26T12:29:57.943318800Z"
    }
   },
   "id": "e9bcd6409f859e0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f0c6e6e7b6d637"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
